{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Algorithm Evaluation for Fraud Detection\n",
        "\n",
        "We evaluate a set of widely used machine learning algorithms that are suitable for structured, tabular,\n",
        "and highly imbalanced fraud-detection data. Each algorithm offers different strengths in terms of\n",
        "interpretability, robustness, and detection capability.\n",
        "\n",
        "Below are the selected models and the reasons for including them:\n",
        "\n",
        "---\n",
        "\n",
        "### **1) Logistic Regression**\n",
        "A simple and highly interpretable baseline model.  \n",
        "Useful for understanding linear relationships and providing transparent decision boundaries.\n",
        "\n",
        "---\n",
        "\n",
        "### **2) Decision Tree**\n",
        "Provides clear, rule-based decisions that investigators can easily understand.  \n",
        "Good for identifying important feature splits.\n",
        "\n",
        "---\n",
        "\n",
        "### **3) Random Forest**\n",
        "An ensemble of many decision trees.  \n",
        "Robust to outliers, handles non-linear relationships, and naturally supports class weighting.  \n",
        "Commonly used in fraud detection due to strong performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **4) Gradient Boosting (e.g., XGBoost / GradientBoostingClassifier)**\n",
        "Builds trees sequentially, correcting previous errors.  \n",
        "High accuracy, excels at complex fraud patterns, and generally outperforms simpler models.\n",
        "\n",
        "---\n",
        "\n",
        "### **5) Support Vector Machine (SVM)**\n",
        "Effective in high-dimensional spaces and good for detecting subtle patterns.  \n",
        "Works well when the minority class has unique boundaries.\n",
        "\n",
        "---\n",
        "\n",
        "Evaluating these algorithms helps us compare interpretability vs. predictive power and select the most\n",
        "appropriate model for fraud detection.\n"
      ],
      "metadata": {
        "id": "zS5psb-YzPmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  FIXED: ALGORITHM EVALUATION FOR FRAUD DETECTION\n",
        "# ============================================================\n",
        "\n",
        "# ---------- Imports ----------\n",
        "from imblearn.pipeline import Pipeline        # <-- IMPORTANT FIX\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create SMOTE instance\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# ============================================================\n",
        "# Logistic Regression\n",
        "# ============================================================\n",
        "log_reg = Pipeline([\n",
        "    ('smote', SMOTE(random_state=42)),    # Oversampling\n",
        "    ('model', LogisticRegression(\n",
        "        class_weight='balanced',\n",
        "        max_iter=500,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred_lr = log_reg.predict(X_test)\n",
        "y_proba_lr = log_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Decision Tree\n",
        "# ============================================================\n",
        "tree = Pipeline([\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('model', DecisionTreeClassifier(\n",
        "        class_weight='balanced',\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "tree.fit(X_train, y_train)\n",
        "y_pred_tree = tree.predict(X_test)\n",
        "y_proba_tree = tree.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Random Forest\n",
        "# ============================================================\n",
        "rf = Pipeline([\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('model', RandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Gradient Boosting\n",
        "# ============================================================\n",
        "gb = Pipeline([\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('model', GradientBoostingClassifier(\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "y_proba_gb = gb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SVM\n",
        "# ============================================================\n",
        "svm_model = Pipeline([\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('model', SVC(\n",
        "        class_weight='balanced',\n",
        "        probability=True,\n",
        "        kernel='rbf',\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "y_proba_svm = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PRINT METRICS\n",
        "# ============================================================\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": (y_pred_lr, y_proba_lr),\n",
        "    \"Decision Tree\": (y_pred_tree, y_proba_tree),\n",
        "    \"Random Forest\": (y_pred_rf, y_proba_rf),\n",
        "    \"Gradient Boosting\": (y_pred_gb, y_proba_gb),\n",
        "    \"SVM\": (y_pred_svm, y_proba_svm)\n",
        "}\n",
        "\n",
        "for name, (pred, proba) in models.items():\n",
        "    print(f\"\\n==================== {name} ====================\")\n",
        "    print(classification_report(y_test, pred))\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    print(\"PR-AUC:\", pr_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hESvbfTxzX3O",
        "outputId": "a14a82ab-3705-4816-c8be-54f1f10e7e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== Logistic Regression ====================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.89      0.94       981\n",
            "           1       0.45      0.86      0.59       101\n",
            "\n",
            "    accuracy                           0.89      1082\n",
            "   macro avg       0.72      0.88      0.76      1082\n",
            "weighted avg       0.93      0.89      0.90      1082\n",
            "\n",
            "PR-AUC: 0.7429020451837107\n",
            "\n",
            "==================== Decision Tree ====================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.91      0.93       981\n",
            "           1       0.41      0.59      0.48       101\n",
            "\n",
            "    accuracy                           0.88      1082\n",
            "   macro avg       0.68      0.75      0.71      1082\n",
            "weighted avg       0.90      0.88      0.89      1082\n",
            "\n",
            "PR-AUC: 0.5200577311871291\n",
            "\n",
            "==================== Random Forest ====================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95       981\n",
            "           1       0.50      0.72      0.59       101\n",
            "\n",
            "    accuracy                           0.91      1082\n",
            "   macro avg       0.74      0.82      0.77      1082\n",
            "weighted avg       0.93      0.91      0.91      1082\n",
            "\n",
            "PR-AUC: 0.6986984008680994\n",
            "\n",
            "==================== Gradient Boosting ====================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.88      0.93       981\n",
            "           1       0.41      0.81      0.54       101\n",
            "\n",
            "    accuracy                           0.87      1082\n",
            "   macro avg       0.69      0.85      0.74      1082\n",
            "weighted avg       0.93      0.87      0.89      1082\n",
            "\n",
            "PR-AUC: 0.7124316082028233\n",
            "\n",
            "==================== SVM ====================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.85      0.91       981\n",
            "           1       0.38      0.89      0.53       101\n",
            "\n",
            "    accuracy                           0.85      1082\n",
            "   macro avg       0.68      0.87      0.72      1082\n",
            "weighted avg       0.93      0.85      0.88      1082\n",
            "\n",
            "PR-AUC: 0.4743931651544512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Model Comparison by Interpretability, Computational Cost, Robustness to Imbalance, and Suitability for Mixed Data\n",
        "\n",
        "To determine which algorithm best fits the fraud detection problem, we evaluated each model according\n",
        "to four critical dimensions:\n",
        "\n",
        "---\n",
        "\n",
        "##  Interpretability\n",
        "Interpretability is essential in healthcare fraud detection since investigators must explain why a\n",
        "provider was flagged.\n",
        "\n",
        "- **Highly interpretable:** Logistic Regression, Decision Tree  \n",
        "- **Moderately interpretable:** Random Forest (via feature importance)  \n",
        "- **Low interpretability:** Gradient Boosting, SVM  \n",
        "\n",
        "---\n",
        "\n",
        "##  Computational Feasibility\n",
        "- **Fastest models:** Logistic Regression, Decision Tree  \n",
        "- **Moderate cost:** Random Forest  \n",
        "- **Heaviest models:** Gradient Boosting and SVM (especially with nonlinear kernels)  \n",
        "\n",
        "Logistic Regression and Random Forest are computationally efficient enough for large-scale deployment.\n",
        "\n",
        "---\n",
        "\n",
        "##  Robustness to Class Imbalance\n",
        "Given the strong imbalance in fraud detection datasets:\n",
        "\n",
        "- **Most robust:** Logistic Regression & Random Forest  \n",
        "  - Both support class weighting and work well with SMOTE  \n",
        "- **Moderate:** Gradient Boosting  \n",
        "- **Weak:** Decision Tree and SVM showed lower precision and PR-AUC  \n",
        "\n",
        "---\n",
        "\n",
        "##  Suitability for Mixed Data\n",
        "Your aggregated dataset combines numerical, categorical, and engineered features.\n",
        "\n",
        "- **Tree-based models (RF, GB, DT)** naturally handle mixed data well  \n",
        "- **Logistic Regression & SVM** require encoding and scaling, but still performed strongly\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "Random Forest and Logistic Regression offer the best combination of interpretability, computational\n",
        "efficiency, robustness to imbalance, and adaptability to mixed data. Logistic Regression performed\n",
        "exceptionally well despite preprocessing requirements.\n"
      ],
      "metadata": {
        "id": "RWqmMFgnziqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Justification of the Primary Model Choice\n",
        "\n",
        "Based on the empirical performance metrics and alignment with dataset characteristics, **Logistic\n",
        "Regression is selected as the primary fraud detection model**.\n",
        "\n",
        "---\n",
        "\n",
        "##  Best Performance on Imbalanced Metrics\n",
        "Logistic Regression achieved the **highest PR-AUC (0.743)** among all models.  \n",
        "Since PR-AUC is the most important metric in imbalanced problems, this indicates superior ranking\n",
        "performance for fraudulent cases.\n",
        "\n",
        "It also achieved:\n",
        "- **Recall = 0.86** (second highest overall)\n",
        "- **F1-score = 0.59** (tied for highest)\n",
        "\n",
        "This demonstrates strong ability to detect fraud with balanced precision–recall behavior.\n",
        "\n",
        "---\n",
        "\n",
        "##  Interpretability (Critical for Medicare Compliance)\n",
        "Fraud detection requires clear explanations for flagged providers.  \n",
        "Logistic Regression provides:\n",
        "- transparent coefficients  \n",
        "- easy-to-understand feature contributions  \n",
        "- simple threshold-based decisions  \n",
        "\n",
        "This makes it ideal for auditing and regulatory environments.\n",
        "\n",
        "---\n",
        "\n",
        "##  Alignment with Dataset Characteristics\n",
        "Your dataset shows strong linear separation in engineered features, which Logistic Regression can model\n",
        "effectively.  \n",
        "It also works extremely well with:\n",
        "- SMOTE oversampling  \n",
        "- class_weight balancing  \n",
        "- aggregated numerical features  \n",
        "\n",
        "This contributes to its strong PR-AUC and recall.\n",
        "\n",
        "---\n",
        "\n",
        "##  Final Statement\n",
        "Given its superior PR-AUC, strong recall, interpretability, computational efficiency, and alignment\n",
        "with dataset structure, **Logistic Regression is selected as the primary model**.  \n",
        "Random Forest is used as the secondary comparison model due to its high precision and robustness.\n"
      ],
      "metadata": {
        "id": "zFYWb9BR5-N4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Implementation of Additional Comparison Models\n",
        "\n",
        "To benchmark the performance of the primary model (Logistic Regression), we implemented two additional\n",
        "models commonly used in fraud detection:\n",
        "\n",
        "#### ** Random Forest Classifier**\n",
        "Selected for its robustness, ability to model complex interactions, and high precision on our dataset.\n",
        "It serves as a strong ensemble-based baseline.\n",
        "\n",
        "#### ** Gradient Boosting Classifier**\n",
        "Chosen for its ability to capture non-linear relationships and sequentially improve on weak learners.\n",
        "It provides a high-recall alternative and strong ranking ability.\n",
        "\n",
        "Both comparison models were trained under the same conditions as the primary model using:\n",
        "- SMOTE oversampling  \n",
        "- class weighting (when applicable)  \n",
        "- identical train–test splits and evaluation metrics  \n",
        "\n",
        "This ensures a fair and consistent comparison of model performance.\n"
      ],
      "metadata": {
        "id": "iY086f2W6kqn"
      }
    }
  ]
}